# -*- coding: utf-8 -*-
"""nl-to-sql.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/charanhu/NLQ_to_Dashboard/blob/master/nl-to-sql.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in

import joblib
import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
import string
from string import digits
import matplotlib.pyplot as plt
# %matplotlib inline
import re

import seaborn as sns
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from keras.layers import Input, LSTM, Embedding, Dense
from keras.models import Model

print(os.listdir("../input"))

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
pd.set_option('display.max_colwidth', -1)

# Any results you write to the current directory are saved as output.

lines = pd.read_csv("../input/wikisql/train.csv", encoding='utf-8')

lines.head(20)

pd.isnull(lines).sum()

lines.drop_duplicates(inplace=True)

"""* ### Let us pick any 35000 rows from the dataset."""

lines = lines.sample(n=35000, random_state=42)
lines.shape

# Lowercase all characters
lines['question'] = lines['question'].apply(lambda x: x.lower())
lines['sql'] = lines['sql'].apply(lambda x: x.lower())

# Remove extra spaces
lines['question'] = lines['question'].apply(lambda x: x.strip())
lines['sql'] = lines['sql'].apply(lambda x: x.strip())
lines['question'] = lines['question'].apply(lambda x: re.sub(" +", " ", x))
lines['sql'] = lines['sql'].apply(lambda x: re.sub(" +", " ", x))

# Add start and end tokens to target sequences
lines['sql'] = lines['sql'].apply(lambda x: 'START_ ' + x + ' _END')

lines.head()

# Get Question and SQL Vocab
all_questions_words = set()
for question in lines['question']:
    for word in question.split():
        if word not in all_questions_words:
            all_questions_words.add(word)

all_sql_words = set()
for sql in lines['sql']:
    for word in sql.split():
        if word not in all_sql_words:
            all_sql_words.add(word)

len(all_questions_words)

len(all_sql_words)

lines['length_question'] = lines['question'].apply(
    lambda x: len(x.split(" ")))  # Number of words in a question
lines['length_sql'] = lines['sql'].apply(
    lambda x: len(x.split(" ")))  # Number of words in a sql

lines.head()

lines[lines['length_question'] > 30].shape

lines = lines[lines['length_question'] <= 20]
lines = lines[lines['length_sql'] <= 20]

lines.shape

print("maximum length of SQL Sentence ", max(lines['length_sql']))
print("maximum length of Question Sentence ", max(lines['length_question']))

max_length_src = max(lines['length_sql'])
max_length_tar = max(lines['length_question'])

input_words = sorted(list(all_eng_words))
target_words = sorted(list(all_hindi_words))
num_encoder_tokens = len(all_eng_words)
num_decoder_tokens = len(all_hindi_words)
num_encoder_tokens, num_decoder_tokens

num_decoder_tokens += 1  # for zero padding

input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])
target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])

reverse_input_char_index = dict((i, word)
                                for word, i in input_token_index.items())
reverse_target_char_index = dict((i, word)
                                 for word, i in target_token_index.items())

lines = shuffle(lines)
lines.head(30)

"""### Split the data into train and test"""

X, y = lines['question'], lines['sql']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape

"""### Let us save this data"""

X_train.to_pickle('X_train.pkl')
X_test.to_pickle('X_test.pkl')


def generate_batch(X=X_train, y=y_train, batch_size=128):
    ''' Generate a batch of data '''
    while True:
        for j in range(0, len(X), batch_size):
            encoder_input_data = np.zeros(
                (batch_size, max_length_src), dtype='float32')
            decoder_input_data = np.zeros(
                (batch_size, max_length_tar), dtype='float32')
            decoder_target_data = np.zeros(
                (batch_size, max_length_tar, num_decoder_tokens), dtype='float32')
            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):
                for t, word in enumerate(input_text.split()):
                    # encoder input seq
                    encoder_input_data[i, t] = input_token_index[word]
                for t, word in enumerate(target_text.split()):
                    if t < len(target_text.split())-1:
                        # decoder input seq
                        decoder_input_data[i, t] = target_token_index[word]
                    if t > 0:
                        decoder_target_data[i, t - 1,
                                            target_token_index[word]] = 1.
            yield([encoder_input_data, decoder_input_data], decoder_target_data)


"""### Encoder-Decoder Architecture"""

latent_dim = 300

# Encoder
encoder_inputs = Input(shape=(None,))
enc_emb = Embedding(num_encoder_tokens, latent_dim,
                    mask_zero=True)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
# We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)
dec_emb = dec_emb_layer(decoder_inputs)
# We set up our decoder to return full output sequences,
# and to return internal states as well. We don't use the
# return states in the training model, but we will use them in inference.
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb,
                                     initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

model.summary()

train_samples = len(X_train)
val_samples = len(X_test)
batch_size = 64
epochs = 20

model.fit_generator(generator=generate_batch(X_train, y_train, batch_size=batch_size),
                    steps_per_epoch=train_samples//batch_size,
                    epochs=epochs,
                    validation_data=generate_batch(
                        X_test, y_test, batch_size=batch_size),
                    validation_steps=val_samples//batch_size)

# save the model to disk
filename = 'model.sav'
joblib.dump(model, filename)

model.save_weights('nmt_weights.h5')

model.summary()

# Encode the input sequence to get the "thought vectors"
encoder_model = Model(encoder_inputs, encoder_states)

# Decoder setup
# Below tensors will hold the states of the previous time step
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

# Get the embeddings of the decoder sequence
dec_emb2 = dec_emb_layer(decoder_inputs)

# To predict the next word in the sequence, set the initial states to the states from the previous time step
decoder_outputs2, state_h2, state_c2 = decoder_lstm(
    dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]
# A dense softmax layer to generate prob dist. over the target vocabulary
decoder_outputs2 = decoder_dense(decoder_outputs2)

# Final decoder model
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs2] + decoder_states2)


def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)
    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0] = target_token_index['START_']

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict(
            [target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += ' '+sampled_char

        # Exit condition: either hit max length
        # or find stop character.
        if (sampled_char == '_END' or
           len(decoded_sentence) > 200):
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index

        # Update states
        states_value = [h, c]

    return decoded_sentence


train_gen = generate_batch(X_train, y_train, batch_size=1)

k = -1

f_question_file = open("question.txt", "w")
sql_file = open("sql.txt", "w")
predection_file = open("predection.txt", "w")

df = pd.DataFrame(columns=['Input qustion sentence',
                  'Actual sql Translation', 'Prideccted'])

for x in range(10000):
    k += 1
    (input_seq, actual_output), _ = next(train_gen)
    decoded_sentence = decode_sequence(input_seq)
    data_dict = {
        "Input qustion sentence": X_train[k:k+1].values[0],
        "Actual sql Translation": y_train[k:k+1].values[0][6:-4],
        "Prideccted": decoded_sentence[:-4]
    }

    df = df.append(data_dict, ignore_index=True)

    f_question_file = open("question.txt", "a")
    sql_file = open("sql.txt", "a")
    predection_file = open("predection.txt", "a")

    f_question_file.write(X_train[k:k+1].values[0]+'\n')
    sql_file.write(y_train[k:k+1].values[0][6:-4]+'\n')
    predection_file.write(decoded_sentence[:-4]+'\n')

    f_question_file.close()
    sql_file.close()
    predection_file.close()
df.to_csv('resultMachine.csv')
